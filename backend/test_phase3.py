#!/usr/bin/env python3
"""
Test script for Phase 3: Difficulty-Adaptive Question Generation

This script tests the enhanced question generation functionality including:
- FSRS difficulty-based question type selection
- Question type variety and progression
- Context-aware question generation
- Sophisticated answer quality analysis
- Adaptive scoring with question type weighting
"""

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), 'my_agent'))

import asyncio
from my_agent.utils.firebase_service import Task
from my_agent.utils.state import GraphState
from my_agent.utils.tools import (
    get_task_context, select_question_type, generate_question_by_type,
    analyze_answer_quality, calculate_adaptive_score, QUESTION_TYPES,
    call_main_llm, call_evaluator_llm
)

def test_question_type_definitions():
    """Test that all question types are properly defined"""
    print("üß™ Testing question type definitions...")
    
    expected_types = ["free_recall", "cued_recall", "recognition", "application", 
                      "connection", "elaboration", "analysis", "comparison"]
    
    for q_type in expected_types:
        if q_type in QUESTION_TYPES:
            info = QUESTION_TYPES[q_type]
            print(f"  ‚úÖ {q_type}: {info['difficulty']} - {info['description'][:50]}...")
        else:
            print(f"  ‚ùå Missing question type: {q_type}")
    
    print(f"  üìä Total question types defined: {len(QUESTION_TYPES)}")

def test_task_context_extraction():
    """Test task context extraction for different scenarios"""
    print("\nüß™ Testing task context extraction...")
    
    # Mock tasks with different difficulty levels
    mock_tasks = [
        Task("high_difficulty_task", 0.9, "2025-01-01", "2025-01-10", 2, 1, 1.0, 15),
        Task("medium_task", 0.5, "2025-01-05", "2025-01-14", 3, 2, 1.5, 5),
        Task("easy_task", 0.1, "2025-01-08", "2025-01-16", 5, 3, 2.0, 2),
    ]
    
    # Test due_items session
    due_items_state = {
        "session_type": "due_items",
        "tasks": mock_tasks,
        "topics": ["high_difficulty_task", "medium_task", "easy_task"]
    }
    
    for topic in due_items_state["topics"]:
        context = get_task_context(due_items_state, topic)
        print(f"  üìã {topic}: difficulty={context['difficulty']:.2f}, overdue={context['days_overdue']} days")
    
    # Test custom_topics session (should return defaults)
    custom_state = {
        "session_type": "custom_topics",
        "topics": ["python", "react"]
    }
    
    context = get_task_context(custom_state, "python")
    print(f"  üéØ Custom topic 'python': difficulty={context['difficulty']:.2f} (default)")

def test_difficulty_based_question_selection():
    """Test question type selection based on difficulty levels"""
    print("\nüß™ Testing difficulty-based question selection...")
    
    difficulty_scenarios = [
        (0.95, "Very high difficulty - should prefer easy questions"),
        (0.75, "High difficulty - should prefer easier questions"),
        (0.5, "Medium difficulty - should be balanced"),
        (0.25, "Low difficulty - should prefer harder questions"),
        (0.05, "Very low difficulty - should prefer hardest questions")
    ]
    
    for difficulty, description in difficulty_scenarios:
        # Test multiple selections to see variety
        selections = []
        for i in range(10):
            question_type = select_question_type(difficulty, i+1, [])
            selections.append(question_type)
        
        unique_types = list(set(selections))
        print(f"  üéØ {description}")
        print(f"     Difficulty {difficulty:.2f}: Generated types: {unique_types}")

def test_question_progression():
    """Test question type progression and variety within a topic"""
    print("\nüß™ Testing question progression and variety...")
    
    # Simulate a medium-difficulty topic progression
    difficulty = 0.6
    topic_history = []
    
    print(f"  üìö Simulating progression for medium difficulty ({difficulty}) topic:")
    
    for question_num in range(1, 6):
        # Add mock previous Q&A to history
        if question_num > 1:
            topic_history.append({
                "question": f"Previous question {question_num-1}",
                "answer": "Mock answer",
                "topic": "test_topic",
                "question_type": last_type
            })
        
        question_type = select_question_type(difficulty, question_num, topic_history)
        last_type = question_type
        
        print(f"    Q{question_num}: {question_type} ({QUESTION_TYPES[question_type]['difficulty']})")

def test_context_aware_question_generation():
    """Test context-aware question generation"""
    print("\nüß™ Testing context-aware question generation...")
    
    topic = "machine_learning"
    
    # Test different question types with various contexts
    test_scenarios = [
        {
            "type": "free_recall",
            "context": {"repetition": 5, "days_overdue": 3},
            "description": "High repetition, slightly overdue"
        },
        {
            "type": "cued_recall", 
            "context": {"repetition": 1, "days_overdue": 14},
            "description": "Low repetition, very overdue"
        },
        {
            "type": "application",
            "context": {"repetition": 2, "days_overdue": 0},
            "description": "Medium repetition, on time"
        }
    ]
    
    for scenario in test_scenarios:
        question = generate_question_by_type(
            topic, 
            scenario["type"], 
            1, 
            [], 
            scenario["context"]
        )
        print(f"  üéØ {scenario['type']} ({scenario['description']}):")
        print(f"     '{question[:80]}...'")

def test_answer_quality_analysis():
    """Test answer quality analysis heuristics"""
    print("\nüß™ Testing answer quality analysis...")
    
    test_answers = [
        {
            "answer": "I don't know.",
            "description": "Very poor answer"
        },
        {
            "answer": "Machine learning is about algorithms.",
            "description": "Basic answer"
        },
        {
            "answer": "Machine learning is a method for teaching computers to learn patterns from data. For example, we can use supervised learning to classify images.",
            "description": "Good answer with example"
        },
        {
            "answer": "Machine learning is a crucial subset of artificial intelligence that focuses on developing algorithms and statistical models. The primary approach involves training systems on data to identify patterns and make predictions. For example, supervised learning uses labeled datasets to teach models, while unsupervised learning finds hidden patterns. The key principles include feature selection, model validation, and avoiding overfitting through techniques like cross-validation.",
            "description": "Excellent comprehensive answer"
        }
    ]
    
    for test in test_answers:
        quality = analyze_answer_quality(test["answer"])
        total_score = sum(quality.values()) / len(quality)
        
        print(f"  üìù {test['description']}:")
        print(f"     Total quality: {total_score:.2f}")
        print(f"     Breakdown: {quality}")

def test_adaptive_scoring():
    """Test adaptive scoring with different difficulty and question type combinations"""
    print("\nüß™ Testing adaptive scoring...")
    
    # Mock Q&A pairs with different question types
    topic_qa = [
        {
            "question": "What do you know about python?",
            "answer": "Python is a programming language that's easy to learn and powerful for data science.",
            "question_type": "free_recall"
        },
        {
            "question": "Is python good for beginners?",
            "answer": "Yes",
            "question_type": "recognition"
        },
        {
            "question": "Give me an example of using python.",
            "answer": "You can use Python for web development with frameworks like Django, for data analysis with pandas, or for machine learning with scikit-learn.",
            "question_type": "application"
        }
    ]
    
    # Mock question types tracking
    question_types = [
        {"topic": "python", "type": "free_recall", "question": "What do you know about python?"},
        {"topic": "python", "type": "recognition", "question": "Is python good for beginners?"},
        {"topic": "python", "type": "application", "question": "Give me an example of using python."}
    ]
    
    # Test with different difficulty levels
    difficulty_tests = [
        {"difficulty": 0.9, "days_overdue": 20, "description": "High difficulty, very overdue"},
        {"difficulty": 0.5, "days_overdue": 5, "description": "Medium difficulty, slightly overdue"},
        {"difficulty": 0.1, "days_overdue": 0, "description": "Low difficulty, on time"}
    ]
    
    for test in difficulty_tests:
        task_context = {
            "difficulty": test["difficulty"],
            "days_overdue": test["days_overdue"],
            "stability": 1.0,
            "repetition": 2,
            "previous_interval": 3
        }
        
        score = calculate_adaptive_score("python", topic_qa, task_context, question_types)
        print(f"  üèÜ {test['description']}: Score = {score}/5")

def test_integrated_question_generation():
    """Test the complete integrated question generation flow"""
    print("\nüß™ Testing integrated question generation flow...")
    
    # Create mock state for due_items session
    mock_tasks = [
        Task("sleep_hygiene", 0.8, "2025-01-01", "2025-01-10", 2, 1, 1.0, 12),
        Task("exercise_routine", 0.3, "2025-01-05", "2025-01-15", 4, 3, 2.0, 3)
    ]
    
    state = {
        "session_type": "due_items",
        "user_id": "test_user",
        "topics": ["sleep_hygiene", "exercise_routine"],
        "tasks": mock_tasks,
        "current_topic_index": 0,
        "question_count": 1,
        "history": [],
        "question_types": [],
        "user_input": None,
        "next_question": None,
        "done": False,
        "scores": None,
        "max_topics": 2,
        "max_questions": 3
    }
    
    print(f"  üéØ Testing question generation for high-difficulty topic 'sleep_hygiene'...")
    
    # Generate first question
    question1 = call_main_llm(state)
    print(f"     Q1: '{question1[:60]}...'")
    
    # Simulate answering and generate second question
    state["question_count"] = 2
    state["history"].append({
        "question": question1,
        "answer": "Sleep hygiene involves maintaining good sleep habits like consistent bedtime.",
        "topic": "sleep_hygiene",
        "question_type": "cued_recall"
    })
    
    question2 = call_main_llm(state)
    print(f"     Q2: '{question2[:60]}...'")
    
    print(f"  üìä Question types generated: {[qt['type'] for qt in state.get('question_types', [])]}")

def test_complete_evaluation_flow():
    """Test the complete evaluation flow with enhanced scoring"""
    print("\nüß™ Testing complete evaluation flow...")
    
    # Create a complete session state
    mock_tasks = [
        Task("python_basics", 0.7, "2025-01-01", "2025-01-12", 2, 1, 1.0, 8),
        Task("data_structures", 0.4, "2025-01-05", "2025-01-15", 3, 2, 1.5, 2)
    ]
    
    state = {
        "session_type": "due_items",
        "user_id": "test_user",
        "topics": ["python_basics", "data_structures"],
        "tasks": mock_tasks,
        "history": [
            {
                "question": "What do you know about python basics?",
                "answer": "Python is a high-level programming language with simple syntax. It's interpreted and supports multiple programming paradigms.",
                "topic": "python_basics",
                "question_type": "free_recall"
            },
            {
                "question": "Is python good for beginners?",
                "answer": "Yes, because of its readable syntax.",
                "topic": "python_basics", 
                "question_type": "recognition"
            },
            {
                "question": "What are data structures?",
                "answer": "Data structures are ways to organize data. Examples include lists, dictionaries, sets, and tuples in Python.",
                "topic": "data_structures",
                "question_type": "cued_recall"
            }
        ],
        "question_types": [
            {"topic": "python_basics", "type": "free_recall", "question": "What do you know about python basics?"},
            {"topic": "python_basics", "type": "recognition", "question": "Is python good for beginners?"},
            {"topic": "data_structures", "type": "cued_recall", "question": "What are data structures?"}
        ],
        "done": True
    }
    
    scores = call_evaluator_llm(state)
    
    print(f"  üèÜ Final scores:")
    for topic, score in scores.items():
        task_context = get_task_context(state, topic)
        difficulty = task_context["difficulty"]
        overdue = task_context["days_overdue"]
        print(f"     {topic}: {score}/5 (difficulty: {difficulty:.2f}, overdue: {overdue} days)")

def main():
    """Run all Phase 3 tests"""
    print("üöÄ Phase 3: Difficulty-Adaptive Question Generation Tests")
    print("=" * 70)
    
    try:
        # Run all tests
        test_question_type_definitions()
        test_task_context_extraction()
        test_difficulty_based_question_selection()
        test_question_progression()
        test_context_aware_question_generation()
        test_answer_quality_analysis()
        test_adaptive_scoring()
        test_integrated_question_generation()
        test_complete_evaluation_flow()
        
        print("\n" + "=" * 70)
        print("‚úÖ All Phase 3 tests passed!")
        print("\nüìã Phase 3 Implementation Summary:")
        print("  ‚úÖ 8 sophisticated question types with learning rationale")
        print("  ‚úÖ FSRS difficulty-based question type selection")
        print("  ‚úÖ Question progression and variety enforcement")
        print("  ‚úÖ Context-aware question generation (overdue, repetition)")
        print("  ‚úÖ Multi-factor answer quality analysis")
        print("  ‚úÖ Question type difficulty weighting in scoring")
        print("  ‚úÖ Adaptive scoring with difficulty bonuses")
        print("  ‚úÖ Overdue-sensitive scoring adjustments")
        print("  ‚úÖ Enhanced evaluation metadata")
        print("\nüéØ Ready for production testing and user feedback!")
        
    except Exception as e:
        print(f"\n‚ùå Test failed: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 